
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Samiha Mirza</title>
  <link rel="stylesheet" href="style.css">
</head>

<body>
  <div class="container">

    <header>
      <h1>Samiha Mirza</h1>
      <p class="subtitle">
        PhD Candidate in Computer Science · Applied Machine Learning · Data-Centric AI
      </p>
      <p class="links">
        <a href="mailto:samiha.mirza1234@gmail.com">Email</a> ·
        <a href="https://www.linkedin.com/in/samiha-mirza-a437b2160">LinkedIn</a> ·
        <a href="https://scholar.google.com/citations?user=R1Iy2mAAAAAJ&hl=en&oi=ao">Scholar</a> ·
        <a href="Samiha_2026_Resume_T.pdf">CV</a>
      </p>
    </header>

    <section>
      <h2>About Me</h2>
      <p>
        I am a PhD candidate in Computer Science at the <a href="https://www.uh.edu/">University of Houston</a>, advised by <a href="https://shishirkshah.github.io/">Dr. Shihsir Shah</a> working at the
        intersection of applied machine learning, representation learning, and robustness
        for real-world, high-variability datasets.
      </p>
      <p>
        My research focuses on <b>data-centric AI</b>: understanding how data quality,
        distribution shift, and structural variability influence model behavior. I design
        experimental frameworks and learning strategies that improve generalization,
        stability, and interpretability of deep learning systems, particularly for
        imaging and perception problems.
      </p>
      <p>
        I have industry research experience at <a href="https://www.shell.us/">Shell</a>, where I built end-to-end ML pipelines,
        conducted controlled experiments, and translated model outputs into actionable
        decision-support tools.
      </p>
    </section>

    <section>
      <h2>Education</h2>
      <ul>
        <li>
          <b>PhD in Computer Science</b>, University of Houston <br>
          <span class="muted">Aug 2022 – May 2026 (expected)</span>
        </li>
        <li>
          <b>MS in Computer Science</b>, University of Houston <br>
          <span class="muted">June 2025</span>
        </li>
      </ul>
    </section>

    <section>
  <h2>Selected Projects</h2>

  <!-- Project 1: Continual Learning / KD -->
  <div class="project-row">
    <div class="project-media">
      <img src="assets/KD_Paper_images-Overall_architecture.jpg" alt="Knowledge distillation teacher-student framework">
    </div>
    <div class="project-text">
      <h3>Continual Learning via Knowledge Distillation for Robust Segmentation</h3>
      <p class="project-keywords">
        Continual Learning · Knowledge Distillation · Distribution Shift
      </p>
      <p>
        This work investigates continual learning strategies for semantic segmentation under realistic data evolution,
        where models are incrementally updated with new data distributions while retaining performance on previously
        seen domains. We focus on mitigating catastrophic forgetting in settings where full data replay is infeasible.
      </p>
      <p>
        We use a teacher–student distillation framework in which a frozen prior model guides optimization of a student
        during later stages via a weighted loss that combines supervised learning on current data with a distillation
        objective that preserves prior behavior.
      </p>
      <!-- Optional links -->
      <!-- <p class="project-links"><a href="#">Paper</a> · <a href="#">Code</a></p> -->
    </div>
  </div>

  <!-- Project 2: Autoencoder -->
  <div class="project-row">
    <div class="project-media">
      <img src="assets/overall_pipeline.jpg" alt="Autoencoder latent space pipeline">
    </div>
    <div class="project-text">
      <h3>Autoencoder-Based Latent Space Analysis for Data Selection</h3>
      <p class="project-keywords">
        Representation Learning · Latent Spaces · Data Quality
      </p>
      <p>
        We train an autoencoder to learn compact latent embeddings that capture dominant structural and textural patterns,
        and then use the encoder as a fixed feature extractor to project samples into a shared latent space.
      </p>
      <p>
        Latent-space structure is analyzed to identify outliers and underrepresented modes. A lightweight classifier
        (e.g., SVM) supports principled selection of informative samples for downstream segmentation, improving robustness
        under variability.
      </p>
    </div>
  </div>

  <!-- Project 3: EAGE / Data selection -->
  <div class="project-row">
    <div class="project-media">
      <img src="assets/frequency_based.jpg" alt="Feature learning + SVM data selection">
    </div>
    <div class="project-text">
      <h3>Data-Centric Quality Assessment for Seismic Interpretation (EAGE)</h3>
      <p class="project-keywords">
        Data-Centric AI · Quality Metrics · Robust Generalization
      </p>
      <p>
        This work studies how seismic data quality and label quality influence salt boundary segmentation under domain
        shift. We evaluate human-defined quality metrics and model-driven representations, and analyze their relationship
        to segmentation accuracy across datasets.
      </p>
      <p>
        Quality-aware data selection is integrated into the training workflow to improve robustness when generalizing to
        unseen domains, highlighting dataset curation as a first-class design choice in seismic interpretation pipelines.
      </p>
    </div>
  </div>

</section>

<footer>
      <p class="muted">
        © 2026 Samiha Mirza · Built with HTML & GitHub Pages
      </p>
    </footer>

  </div>
</body>
</html>